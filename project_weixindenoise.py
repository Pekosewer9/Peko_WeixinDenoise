###   å¾®ä¿¡é™å™ª   ###
##   C:\Users\LENOVO\Desktop\å¾®ä¿¡ç¾¤èš\god_ouselves.csv
##   C:\Users\LENOVO\Desktop\å¾®ä¿¡ç¾¤èš\244210.csv
##   

## current Error:not precise K-means


import csv
import sys
import time
import math
import chardet
import torch
import jieba
from tqdm import tqdm
import random as rd
import numpy as np


from scipy.spatial.distance import cosine
from sentence_transformers import SentenceTransformer,util
from transformers import BertTokenizer, BertForSequenceClassification
from sklearn.cluster import KMeans
from collections import defaultdict
from sklearn.feature_extraction.text import TfidfVectorizer

print("already imported,wait for model")

# åŠ è½½é¢„è®­ç»ƒçš„ Sentence-BERT æ¨¡å‹
# æ¨¡æ‹Ÿæ¨¡å‹åŠ è½½è¿‡ç¨‹
def load_model_with_progress(model_name):
    print(f"Loading model: {model_name}")
    with tqdm(total=100, desc="Loading", unit="%") as pbar:
        model = SentenceTransformer(model_name)
        pbar.update(100)  # æ›´æ–°è¿›åº¦æ¡åˆ° 100%
    # nothing but a good looking
    return model

# ä½¿ç”¨è¿›åº¦æ¡åŠ è½½æ¨¡å‹
model_Bert = load_model_with_progress("paraphrase-MiniLM-L6-v2").to("cuda")

print("model_SBERT injected")

tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=2)

print("model_BertTokenizer injected")

# -I love Midwest Emo
# -Loser

list_data = []
encode_type = {'encoding':"GB2312"}

def get_data(location):
    with open(location,"rb") as file:
        encode_type = chardet.detect(file.read())
    with open(location,"r",encoding='gbk',errors='ignore') as file:
        lines = csv.DictReader(file)
        for line in lines:
            if "<msg>" in line['StrContent']:
                continue
            data = {}
            data['nickname'] = line['NickName']
            data['content'] = line['StrContent']
            data['time'] = line['CreateTime']
            list_data.append(data)

class message:

    def __init__(self,list_data,count_message):
        self.list_data = list_data
        self.count = count_message
    
    def split_words(self):
        for i in range(len(self.list_data)):
            self.list_data[i]['characters'] = list(self.list_data[i]['content'])

    def frame_size(self):
        gama = 50
        time_gap = 300
        return gama

    def one_hot_encode(self,word_list, vocabulary):
        encoding = []
        for word in word_list:
            if word in vocabulary:
                vector = [0] * len(vocabulary)  # åˆå§‹åŒ–å…¨ 0 å‘é‡
                index = vocabulary.index(word)  # æ‰¾åˆ°å­—åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•
                vector[index] = 1  # å°†å¯¹åº”ä½ç½®è®¾ä¸º 1
                encoding.append(vector)
            else:
                # å¦‚æœå­—ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œè®¾ä¸ºå…¨ 0 å‘é‡
                encoding.append([0] * len(vocabulary))
        
        return np.array(encoding)

    def Similarity_compare(self,sentence1,sentence2):
            vocabulary = list(set(sentence2['characters']+sentence1['characters']))
            vec1 = self.one_hot_encode(sentence1['characters'],vocabulary)
            vec2 = self.one_hot_encode(sentence2['characters'],vocabulary)
            norm_vec1 = np.sum(vec1, axis=0)
            norm_vec2 = np.sum(vec2, axis=0)
            dot_product = np.dot(norm_vec1, norm_vec2)
            norm_vec1 = np.linalg.norm(norm_vec1)
            norm_vec2 = np.linalg.norm(norm_vec2)
            similarity = dot_product / (norm_vec1 * norm_vec2) if norm_vec1!= 0 and norm_vec2!= 0 else 0
            # print(f"Similarity betweem {sentence2['content']} and {sentence2['content']} is {similarity}\n")
            # time.sleep(0.5)
            return similarity
    
    def mass_remove(self):
        index = 0
        while index < len(self.list_data):
            sub_list = self.list_data[index:index + 30]
            i = 0
            while i < len(sub_list):  # éå†å­åˆ—è¡¨
                initial_point = sub_list[i]  # è·å–åˆå§‹ç‚¹
                j = i + 1
                while j < len(sub_list):  # éå†åˆå§‹ç‚¹åé¢çš„å…ƒç´ 
                    if self.Similarity_compare(initial_point, sub_list[j]) > 0.85:
                        # print(f"the sentence {initial_point['content']} has been removed with the sentence {sub_list[j]['content']}\n")
                        # time.sleep(0.25)
                        self.list_data.pop(index + j)
                        sub_list.pop(j)
                    else:
                        j += 1  # å¦‚æœä¸æ»¡è¶³æ¡ä»¶ï¼Œç»§ç»­æ£€æŸ¥ä¸‹ä¸€ä¸ªå…ƒç´ 
                i += 1  # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªåˆå§‹ç‚¹
            index += 30

    def meaning_replace():
        
        raise(NotImplementedError)
    
    def Association_calculate(self,sentence1,sentence2):
        inputs = tokenizer(sentence1, sentence2, return_tensors='pt', max_length=128, truncation=True)
        tokens = tokenizer.tokenize(sentence1) + tokenizer.tokenize(sentence2)
        if len(tokens) > 128:
            return 0
        # 3. è¿›è¡Œ NSP é¢„æµ‹
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            # è·å–é¢„æµ‹ç»“æœ
            probs = torch.softmax(logits, dim=1)
            # åˆ¤æ–­æ˜¯å¦è¿è´¯
            coherent_prob = probs[0][1].item()  # 0 åˆ° 1 ä¹‹é—´çš„å€¼
            return coherent_prob
        
            # è®¡ç®—æ˜¯è¿è´¯è¿˜æ˜¯ä¸è¿è´¯
            # is_next = torch.argmax(probs, dim=1).item()  # 0 è¡¨ç¤ºä¸è¿è´¯ï¼Œ1 è¡¨ç¤ºè¿è´¯

        raise(NotImplementedError)
    
    def length_calculate(self,sentence):
        count_words = len(sentence)
        return math.exp(-0.1 * (count_words - 1))
        raise(NotImplementedError)
    
    def X_calculate(self,sentence1,sentence2):
        if sentence1['nickname'] == sentence2['nickname']:
            return 1
        else:
            return 0
        raise(NotImplementedError)
    
    def Distance_calculate(self,index1,index2):
        return ( 1 - (index2 - index1) / 30 )
        raise(NotImplementedError)
    
    def sentence_combine(self,Association_valve,w1,w2,w3,w4):
        frame_len = self.frame_size()
        index = 0
        while index < len(self.list_data) - 30:
            print("move to next point")
            sub_list = self.list_data[index:index + 30]
            innitial_point = sub_list[0]
            j = 1
            while j < len(sub_list):
                if w1*self.Association_calculate(innitial_point['content'],sub_list[j]['content']) + w2*self.length_calculate(sub_list[j]['content']) + w3 * self.X_calculate(innitial_point,sub_list[j]) + w4 * self.Distance_calculate(index,index + j) > Association_valve:
                    if sub_list[j]['content'] != '':
                        # time.sleep(0.5)
                        print(f"combine {self.list_data[index]['content']} with {self.list_data[index + j]['content']}")
                    self.list_data[index]['content'] += ( ';' + self.list_data[index + j]['content'])
                    self.list_data.pop(index + j)
                    sub_list.pop(j)
                else:
                    j += 1
            index += 1
        return 
        raise(NotImplementedError)
    
    def S_Bert_(self):
        for item in self.list_data:
            sentence_embeddings = model_Bert.encode(item['content'])
            item['Sb_vec'] = sentence_embeddings

    def sentence_similarity(self,sentence1,sentence2):
        return (1 - cosine(sentence1['Sb_vec'] , sentence2['Sb_vec']))
        raise(NotImplementedError)
    
    def K_means(self):
        # length_vec = len(self.list_data[0]['Sb_vec'])
        init_centroids = np.zeros_like(self.list_data[0]['Sb_vec'])
        # init_centroids = np.empty((0) * length_vec)
        sum_cluster = 0
        for index in range(len(list_data)-1):
            if self.sentence_similarity(list_data[index],list_data[index+1]) > 0.55:
                init_centroids = np.append(init_centroids,list_data[index+1]['Sb_vec'],axis=0)
                sum_cluster += 1
        # init_centroids dimension Error

        # èšç±»å¯ä»¥æ˜¯sqrtæˆ–è€…log
        cluster_num = math.log(len(self.list_data)) / math.log(2)
        # cluster_num = math.sqrt(len(self.list_data))
        
        cluster_num = math.ceil(cluster_num)
        clustering = SentenceClustering(self.list_data,n_clusters=cluster_num,init_centroids=init_centroids)
        clustered_data , clustered_labels = clustering.cluster_sentences()
        cluster_labels_after = list(set(clustered_labels))
        # å»é‡
        with open("D:\Vscode1\.venv\AI_introduction\lab\output2.txt","w",encoding='utf-8',errors='ignore') as file:
            for label in cluster_labels_after:
                print(f"lable:{label} \n")
                file.write(f"lable:{label} \n")
                for item in clustered_data[label]:
                    print(item['content'])
                    file.write(item['content'] + '\n')
        self.summary(clustered_data , clustered_labels)
        
                    
        return
        raise(NotImplementedError)

    def DBSCAN(self):

        raise(NotImplementedError)
    

    def test(self):
        for data in self.list_data:
            print(data)

    def summary(self , clustered_data , clustered_label):
        used_label = []
        clustered_labels = clustered_label.tolist()
        for label in clustered_labels:
            if label in used_label:
                clustered_labels.pop(label)
                continue
            used_label.append(label)
            print(f"lable:{label}")
            sentence = ""
            for item in clustered_data[label]:
                sentence += item['content']
            words = " ".join(jieba.cut(sentence))  # ä½¿ç”¨ jieba åˆ†è¯ï¼Œå¹¶ç”¨ç©ºæ ¼è¿æ¥
            # è®¡ç®— TF-IDF
            vectorizer = TfidfVectorizer()
            try:
                tfidf_matrix = vectorizer.fit_transform([words])  # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸º TF-IDF çŸ©é˜µ
            except ValueError:
                print("this data is purely stopwords,which is nonsense")
                continue
            # è·å–å…³é”®è¯
            feature_names = vectorizer.get_feature_names_out()
            tfidf_scores = tfidf_matrix.toarray()[0]  # è·å– TF-IDF å€¼

            # æŒ‰ TF-IDF å€¼æ’åºï¼Œæå–æƒé‡æœ€é«˜çš„å…³é”®è¯
            sorted_indices = tfidf_scores.argsort()[::-1]  # ä»é«˜åˆ°ä½æ’åº
            top_n = 3  # æå–å‰ 5 ä¸ªå…³é”®è¯
            if  len(sorted_indices) <=  3:
                print("å­¤ç«‹æ•°æ®ç‚¹ï¼š")
                print(f"{clustered_data['content']}")
            else:
                print("æå–çš„å…³é”®è¯ï¼š")
                for idx in sorted_indices[:top_n]:
                    print(f"{feature_names[idx]}: {tfidf_scores[idx]}")
            clustered_labels.pop(label)
                
class SentenceClustering:
    def __init__(self, list_data, n_clusters,init_centroids=None):
        init_centroids = None
        self.list_data = list_data
        self.n_clusters = n_clusters
        self.init_centroids = init_centroids

        # åˆå§‹åŒ– KMeans
        if init_centroids is not None:
            self.kmeans = KMeans(n_clusters=n_clusters, init=init_centroids, n_init=1)
        else:
            self.kmeans = KMeans(n_clusters=n_clusters)


    def cluster_sentences(self):
        """
        å¯¹å­—å…¸åˆ—è¡¨ä¸­çš„å¥å­è¿›è¡Œ K - means èšç±»
        :return: èšç±»åçš„å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸æ–°å¢ 'cluster_label' é”®ï¼Œè¡¨ç¤ºæ‰€å±ç°‡çš„æ ‡ç­¾
        """
        # æå–æ‰€æœ‰ Sentence - BERT å‘é‡

        sb_vectors = np.array([np.array(item['Sb_vec']) for item in self.list_data])

        # æ£€æŸ¥ sb_vectors çš„å½¢çŠ¶
        print(sb_vectors.shape)  # åº”è¯¥æ˜¯ (n_samples, 384)

        # å¦‚æœåªæœ‰ä¸€ä¸ªæ ·æœ¬ï¼Œæ‰‹åŠ¨ reshape
        if sb_vectors.ndim == 1:
            sb_vectors = sb_vectors.reshape(1, -1)

        # ä½¿ç”¨ K - means èšç±»
        cluster_labels = self.kmeans.fit_predict(sb_vectors)
        # å°†èšç±»æ ‡ç­¾æ·»åŠ åˆ°å­—å…¸ä¸­
        for item, label in zip(self.list_data, cluster_labels):
            item['cluster_label'] = label
            item['cluster_type'] = type.__dictoffset__
        
        grouped_data = defaultdict(list)
        for label, item in zip(cluster_labels,self.list_data):
            grouped_data[label].append(item)

        return grouped_data,cluster_labels

def main():
    location = input("what's location?")
    get_data(location)

    Wechat_message = message(list_data,len(list_data))
    
    Wechat_message.test()

    Wechat_message.split_words()


    print('wait for mass_remove')
    time.sleep(0.5)
    # Wechat_message.mass_remove()

    print('already mass_removed')
    ##Wechat_message.test()
    # Wechat_message.meaning_replace()'
    print('wait for combine')

    start_time = time.time()
    threshold = 0.63
    w1 = 0.75
    w2 = 0.2
    w3 = 0
    w4 = 0.05
    # è®¾ç½®å‚æ•°
    Wechat_message.sentence_combine(threshold,w1,w2,w3,w4)

    print("already combined")
    end_time = time.time()
    elapsed_time = start_time - end_time
    print(f"elapsed time for combine is {elapsed_time}")

    print("wait for S_bert transition")
    Wechat_message.S_Bert_()

    # path = input("K-means(1) or DBSCAN(2)")
    Wechat_message.K_means() ## å…¼summary

    # Wechat_message.DBSCAN()
    if 1==0 :
        print("Invalid inputğŸ¤¯")
        raise(ValueError)


## if __name__ == '__main__':
main()


